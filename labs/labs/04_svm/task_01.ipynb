{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "(com material adaptado do livro texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(clf, axes):\n",
    "    # Constroi uma lista de valores das variáveis independentes\n",
    "    # que cubra o espaço amostral.\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    # Constroi as predições (binárias) e a função de decisão (contínua).\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "\n",
    "    # Desenha a curva de decisão e as curvas de nível da função de decisão.\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"bs\")\n",
    "    plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Equivale a:\n",
    "#\n",
    "# poly_features = PolynomialFeatures(degree=7)\n",
    "# scaler = StandardScaler()\n",
    "# svm_clf = LinearSVC(C=1e-5, dual=False, random_state=RANDOM_SEED)\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=7)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=1e5, dual=False, random_state=RANDOM_SEED)),\n",
    "])\n",
    "\n",
    "# Equivale a:\n",
    "#\n",
    "# X1 = poly_features.fit_transform(X)  # Não faz nada.\n",
    "# X2 = scaler.fit_transform(X1)  # Ajusta media e desvio padrao.\n",
    "# X3 = svm_clf.fit(X2, y)\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "# O polynomial_svm_clf.predict(X) equivale a:\n",
    "#\n",
    "# X1 = poly_features.transform(X)  # Não faz nada.\n",
    "# X2 = scaler.transform(X1)  # Ajusta media e desvio padrao.\n",
    "# X3 = svm_clf.predict(X2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from pprint import pprint\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    polynomial_svm_clf,\n",
    "    {\n",
    "        'svm_clf__C': [10.0**k for k in np.arange(-3, 5, 0.5)],\n",
    "    },\n",
    "    scoring='accuracy',\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=100,\n",
    "        test_size=0.33,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_predictions(grid, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** \n",
    "- Como funciona e para que serve o ShuffleSplit que eu usei no GridSearchCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos analisar a acurácia real do classificador usando validação cruzada, e alterando o parâmetro de regularização $C$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([p['svm_clf__C'] for p in grid.cv_results_['params']])\n",
    "M = grid.cv_results_['mean_test_score']\n",
    "S = grid.cv_results_['std_test_score']\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(f'C = {p:.5f}: mean_accuracy = {m:.3f}, stddev_accuracy = {s:.3f}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('Accuracy from CV', fontsize=20)\n",
    "plt.xlabel(r\"$C$\", fontsize=20)\n",
    "plt.ylabel(r\"$accuracy$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** \n",
    "- Explique o gráfico acima em termos do tradeoff bias/variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Teste o desempenho dos classificadores abaixo no dataset anterior (moons)\n",
    "\n",
    "- LinearSVC\n",
    "- SVC, com kernel:\n",
    "    - polinomial\n",
    "    - RBF\n",
    "   \n",
    "Apresente os seguintes resultados:\n",
    "\n",
    "- parâmetros ótimos\n",
    "- Acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar support vector machines para regressão também.\n",
    "\n",
    "- Em problemas de classificação com SVM queremos construir uma fronteira de decisão tal que a \"avenida de separação\" entre classes é a maior possível. Dentro desta \"avenida\" queremos o menor número de pontos possível.\n",
    "\n",
    "- A idéia de usar SVM para regressão é o contrário: queremos construir um ajuste de função tal que a \"avenida\" contenha o **maior** número de pontos possível, para uma dada largura!\n",
    "\n",
    "Vamos ilustrar estes pontos com um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "m = 100\n",
    "X = 2 * np.random.rand(m, 1) - 1\n",
    "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1) / 10).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg1 = SVR(\n",
    "    kernel=\"poly\",\n",
    "    degree=2,\n",
    "    C=1000,\n",
    "    epsilon=0.1,\n",
    "    gamma='scale',\n",
    ")\n",
    "svm_poly_reg2 = SVR(\n",
    "    kernel=\"poly\",\n",
    "    degree=2,\n",
    "    C=0.001,\n",
    "    epsilon=0.1,\n",
    "    gamma='scale',\n",
    ")\n",
    "svm_poly_reg1.fit(X, y)\n",
    "svm_poly_reg2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_svm_regression(svm_reg, X, y, axes):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
    "    y_pred = svm_reg.predict(x1s)\n",
    "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
    "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
    "    plt.scatter(\n",
    "        X[svm_reg.support_],\n",
    "        y[svm_reg.support_],\n",
    "        s=180,\n",
    "        facecolors='#FFAAAA',\n",
    "    )\n",
    "    plt.plot(X, y, \"bo\")\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.legend(loc=\"upper left\", fontsize=18)\n",
    "    plt.axis(axes)\n",
    "    plt.title(\n",
    "        f\"degree={svm_reg.degree}, \"\n",
    "        f\"C={svm_reg.C}, \"\n",
    "        f\"$\\epsilon$ = {svm_reg.epsilon}\",\n",
    "        fontsize=18,\n",
    "    )\n",
    "    plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Explique o efeito do parâmetro de regularização C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos praticar usando o dataset \"California Housing\" do scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(housing['data'], columns=housing['feature_names'])\n",
    "y = pd.Series(housing['target'], name='MedHouseValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y >= 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.hist(figsize=(15, 15), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tem uns outliers malucos aparentemente! Onde já se viu um distrito onde a ocupação média dos imóveis é mais de 1000 pessoas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.AveOccup > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Descubra o que aconteceu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Parece que essas \"casas\" tem ocupação alta mesmo, ainda mais nos Estados Unidos (e no Brasil). \n",
    " \n",
    "Temos um problema também em relação ao número de cômodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.AveRooms > 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Explique esse fenômeno também"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para não misturar tipos de \"residências\", vamos filtrar o dataset e eliminar alguns outliers. Vamos nos restringir a um número de cômodos menor que 15, e uma ocupação média menor que 10. \n",
    "\n",
    "Vamos também eliminar os distritos onde o valor mediano dos imóveis excede $5.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = (X.AveRooms[:] < 15) & (X.AveOccup < 10) & (y < 5.0)\n",
    "X_filt = X[valid]\n",
    "y_filt = y[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filt.hist(figsize=(12, 12), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_filt.hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_filt,\n",
    "    y_filt,\n",
    "    test_size=0.33,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines são bastante eficientes para conjuntos de dados pequenos, mas seu processo de treinamento é extremamente lento. Para escolher o valor ótimo do parâmetro de regularização $C$ vamos reamostrar os dados e fazer a busca por validação cruzada em um dataset pequeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "X_train_small, y_train_small = resample(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    replace=False,\n",
    "    n_samples=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', SVR(C=1, epsilon=0.1)),\n",
    "    ]),\n",
    "    {\n",
    "        'reg__C': [10**k for k in range(-4, 7)],\n",
    "    },\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=100,\n",
    "        test_size=0.33,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train_small, y_train_small)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [p['reg__C'] for p in grid.cv_results_['params']]\n",
    "M = -grid.cv_results_['mean_test_score']\n",
    "S = np.log(grid.cv_results_['std_test_score'] + 1)\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(p, m, s)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('MSE from CV', fontsize=20)\n",
    "plt.xlabel(r\"$C$\", fontsize=20)\n",
    "plt.ylabel(r\"$y$\", fontsize=20, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Mais uma vez, explique esse gráfico em termos do tradeoff bias/variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar também ajustar o parâmetro $\\gamma$ do modelo (ver 'Gaussian RBF Kernel' no livro texto):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reg', SVR(C=1, epsilon=0.1, gamma=0.1)),\n",
    "    ]),\n",
    "    {\n",
    "        'reg__gamma': [10**k for k in range(-7, 5)],\n",
    "    },\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=100,\n",
    "        test_size=0.33,\n",
    "        random_state=RANDOM_SEED,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid.fit(X_train_small, y_train_small)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [p['reg__gamma'] for p in grid.cv_results_['params']]\n",
    "M = -grid.cv_results_['mean_test_score']\n",
    "S = np.log(grid.cv_results_['std_test_score'] + 1)\n",
    "\n",
    "for p, m, s in zip(P, M, S):\n",
    "    print(p, m, s)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(P, M, S, capsize=4)\n",
    "plt.semilogx()\n",
    "plt.title('MSE from CV', fontsize=20)\n",
    "plt.xlabel(r\"$\\gamma$\", fontsize=20)\n",
    "plt.ylabel(r\"$y$\", fontsize=20, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** yadda yadda yadda tradeoff bias/variance you know what to do :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, vamos testar o desempenho final do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = grid.best_estimator_\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos comparar com um regressor linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lin = lin_reg.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred_lin)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que o regressor SVM é mais preciso, mas requer um tempo de treinamento várias ordens de magnitude maior que o regressor linear.\n",
    "\n",
    "**Atividade:** A unidade de medida da variável dependente é \"dezenas de milhares de dólares\". Explique para seu chefe porque você merece uma promoção pelo seu trabalho com um regressor SVM enquanto seu colega (que não fez a disciplina de Machine Learning) usou uma regressão linear simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividades:**\n",
    "\n",
    "- Qual a idéia fundamental das Support Vector Machines? O que são vetores de suporte?\n",
    "\n",
    "- (Desafio) A segunda idéia mais importante das SVMs é o uso de kernels. Os kernels permitiram a expansão das SVMs para além dos modelos lineares. Em particular, o kernel RBF (radial-basis function) é bastante popular entre os usuários de SVMs, e apresenta desempenho bem elevado em geral. O que são kernels? Qual a sua relação com o problema de otimização dual das SVMs?\n",
    "\n",
    "- Se dobrarmos o número de features em uma modelagem SVM, quanto sobe o tempo de treinamento de um classificador SVM linear? E de um SVM RBF?\n",
    "\n",
    "- Se dobrarmos o número de amostras de treinamento de um classificador SVM linear, quanto sobe o tempo de treinamento? E se for um classificador SVM RBF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
