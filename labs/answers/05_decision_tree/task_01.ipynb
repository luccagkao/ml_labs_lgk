{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "plt.rcParams.update({\"font.size\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árvores de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma árvore de decisão é uma árvore binária onde os vários nós não-terminais correspondem a decisões acerca da amostra sendo analisada, e as folhas correspondem aos valores de predição. Por exemplo, considere o famoso dataset \"Iris\" e o problema de classificar as flores em \"virginica\", \"versicolor\" e \"setosa\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris[\"data\"], columns=iris[\"feature_names\"])\n",
    "y = pd.Series(iris[\"target\"], name=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(X, y):\n",
    "    bins = np.linspace(X.min(), X.max(), 30)\n",
    "    for c in y.value_counts().index:\n",
    "        plt.hist(\n",
    "            X[y == c].values,\n",
    "            bins=bins,\n",
    "            label=str(c),\n",
    "            alpha=0.5,\n",
    "            color=plt.cm.Set1.colors[c],\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_scatter(X, y):\n",
    "    cols = X.columns\n",
    "    n = len(cols)\n",
    "    for i in range(n):\n",
    "        plt.subplot(n, n, i * n + i + 1)\n",
    "        plot_hist(X[cols[i]], y)\n",
    "        plt.xlabel(cols[i])\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            plt.subplot(n, n, i * n + j + 1)\n",
    "            plt.scatter(\n",
    "                X[cols[j]],\n",
    "                X[cols[i]],\n",
    "                c=y,\n",
    "                cmap=plt.cm.Set1,\n",
    "                edgecolor=\"k\",\n",
    "            )\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plot_scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Para simplificar nossa explanação, vamos nos limitar ao uso de apenas duas características: sepal width e petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = X[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_scatter(X_simple, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_simple, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=\"iris_tree.dot\",\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='iris_tree.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que em cada nó o coeficiente Gini diminui em relação ao nó do nível precedente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Estude o material do capítulo 6 e responda:\n",
    "\n",
    "- O que é CART?\n",
    "- O que são modelos paramétricos e não-paramétricos?\n",
    "- Qual o principal hiperparâmetro de regularização de uma árvore de decisão? Porque temos overfitting se este parâmetro não for restrito?\n",
    "- CART consegue fazer tanto classificação (multiclasse inclusive!) como regressão! O que muda no CART entre classificação e regressão?\n",
    "- Explique porque não é necessário re-escalar as *features* quando usamos uma árvore de decisão.\n",
    "- (Ex. 5 Géron): Se para treinar um modelo de árvore de decisão com 1 milhão de amostras levamos 1 hora, quanto tempo a mais levaríamos para treinar um modelo com 10 milhões de amostras?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R:**\n",
    "\n",
    "- O que é CART?\n",
    "\n",
    "CART (Classification and Regression Trees) é um algoritmo de treinamento de árvores de decisão, baseado na noção de impureza do nó, que induz a partição do dataset em cada nó da arvore de modo a diminuir a impureza resultante do particionamento.\n",
    "\n",
    "- O que são modelos paramétricos e não-paramétricos?\n",
    "\n",
    "Modelos paramétricos são aqueles modelos em que o número de parâmetros é fixo, independente do dataset. Modelos não-paramétricos são aqueles em que o número de parâmetros não é pré-determinado, mas sim depende do próprio dataset de treinamento.\n",
    "\n",
    "- Qual o principal hiperparâmetro de regularização de uma árvore de decisão? Porque temos overfitting se este parâmetro não for restrito?\n",
    "\n",
    "O principal parâmetro de regularização de uma árvore de decisão é a profundidade máxima da árvore (max_depth). Caso este parâmetro não seja restrito, a árvore pode expandir até que cada ponto do conjunto de treinamento vire uma partição por si só, caracterizando overfitting.\n",
    "\n",
    "- CART consegue fazer tanto classificação (multiclasse inclusive!) como regressão! O que muda no CART entre classificação e regressão?\n",
    "\n",
    "A medida de impureza.\n",
    "\n",
    "- Explique porque não é necessário re-escalar as *features* quando usamos uma árvore de decisão.\n",
    "\n",
    "Porque no algoritmo CART não importa o valor do corte: apenas a pureza das partições dele resultantes.\n",
    "\n",
    "- (Ex. 5 Géron): Se para treinar um modelo de árvore de decisão com 1 milhão de amostras levamos 1 hora, quanto tempo a mais levaríamos para treinar um modelo com 10 milhões de amostras?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atividade:** Exercício 7 do livro texto (Géron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafio (não precisa entregar)**: Exercício 8 do livro texto (Géron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
