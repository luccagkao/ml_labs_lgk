{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prova Final - Machine Learning\n",
    "\n",
    "**Nome do aluno**: <font color='red'>LUCCA D'OLIVEIRA GHETI KAO</font>\n",
    "\n",
    "Você deve submeter o trabalho via Blackboard. É de sua responsabilidade garantir que o arquivo correto foi enviado.\n",
    "\n",
    "Se você precisou adicionar arquivos extras à essa prova, assegure-se de que você os está enviando também.\n",
    "\n",
    "A prova é individual e com consulta - pode consultar suas anotações, a web, o que quiser, menos perguntar para outros ou para as AIs. Faça o seu trabalho de maneira ética!\n",
    "\n",
    "ChatGPT para perguntas teóricas não é permitido. CoPilot para código é permitido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Temos abaixo uma lista de tópicos de machine learning e uma lista de interlocutores:\n",
    "\n",
    "### Lista de tópicos\n",
    "\n",
    "- LASSO: como funciona, para que serve, como usar (e.g. efeito do parâmetro de regularização).\n",
    "\n",
    "- Support Vector Machines: como funciona hard-SVM e soft-SVM, quando usar (e.g. o problema do número de features versus número de amostras)\n",
    "\n",
    "- Árvore de decisão: como funciona, qual o efeito do número de níveis, que outros hiperparâmetros podem ser experimentados?\n",
    "\n",
    "- Bagging e boosting: para que servem, porque funcionam, qual a diferença entre eles?\n",
    "\n",
    "- A importância da redução de dimensionalidade e o algoritmo PCA\n",
    "\n",
    "- Clustering hierárquico\n",
    "\n",
    "- Autodiff e backpropagation\n",
    "\n",
    "- Machine learning e desigualdade social\n",
    "\n",
    "\n",
    "### Lista de interlocutores\n",
    "\n",
    "- Seu parente mais velho\n",
    "\n",
    "- Seu professor de história do colegial\n",
    "\n",
    "- Uma criança de 12 anos espertinha\n",
    "\n",
    "- Um colega do Insper espertinho\n",
    "\n",
    "- Você mesmo do início do semestre\n",
    "\n",
    "\n",
    "Forme quatro pares tópico-interlocutor, sem repetição de tópico ou de interlocutor, e construa para cada par uma explicação didática do tópico para o respectivo interlocutor. Inclua figuras, código, equações, conforme necessário.\n",
    "\n",
    "Cada par vale 2.5 pontos.\n",
    "\n",
    "#### Rubrica\n",
    "\n",
    "I - não fez ou fez abobrinha\n",
    "\n",
    "D - tem erro na explicação, explicação excessivamente rasa, ou não é adequada para o interlocutor\n",
    "\n",
    "C - explicação correta mas um pouco rasa e pouco adequada ao interlocutor\n",
    "\n",
    "B - explicação bem sólida e com boa didática, adequada ao interlocutor\n",
    "\n",
    "A - explicação e didática excepcionais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação 1\n",
    "\n",
    "Tópico: Árvore de decisão: como funciona, qual o efeito do número de níveis, que outros hiperparâmetros podem ser experimentados?\n",
    "\n",
    "Interlocutor: Seu parente mais velho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vô sabe quando você vai no hortifruti e quer escolher uma melancia, e fica olhando a cor da casca, a rigidez dela, o barulho que ela faz quando você bate e o tamanho dela. Então, sabia que é possível criar um robô que seleciona melancias? Existe um modelo matemático que tem a capacidade de tomar decisões, basta você falar para ele o que é importante, e quais são as perguntas que ele deve se fazer. Vamos pensar assim, dentre todas esses \"testes/perguntas\" que o senhor se faz enquanto escolhe a melancia, qual a mais importante? O tamanho dela, a cor, a rigidez? O que você mais valoriza durante a escolha de uma melancia? Essa noção de importância o robô precisa saber, assim, quando você der uma melancia para ele analisar, o primeiro aspecto que ele vai olhar vai ser o que o senhor considera mais importante. Digamos que seja o tamanho da melancia. O diâmetro dela tem que ser maior que 50 cm. Ao responder essa pergunta, você normalmente já decide se vai comprar a melancia ou não? Se sim, o robô vai interpretar a resposta dessa pergunta como uma decisão, então se o diâmetro for maior que 50 cm, ele vai falar para comprar, se não, porcurar outra. Assim, essa ideia é chamada de Árvore de decisão, e quando contém apenas uma pergunta, ele tem apenas um nível de profundidade, em que o resultado de uma única pergunta, resulta na decisão.\n",
    "\n",
    "Se olharmos somente o tamanho da melancia, muitas vezes vamos escolher ela mal, né? Esse fato de errarmos muitas vezes na escolha por não sabermos direito o que define uma boa melancia é chamado no mundo dos robôs de underfitting, que é basicamente quando seu \"robô\" não aprendeu muito bem com as informações que você passou e ele vai errar diversas vezes na escolha das melancias.\n",
    "\n",
    "Caso você não descarte ela logo de cara pelo seu tamanho devemos pensar em duas perguntas agora - uma para caso ela seja maior que 50 cm e outra caso ela seja menor -, se não, vamos pensar apenas caso ela seja maior e se ela for menor tomamos a decisão de descartá-la. Toda vez que chegamos em um ponto que tomamos decisão, chamamos isso nesse robô \"Árvore de Decisão\" de folha. Toda decisão é uma folha que guarda a informação do que deve ser decidido. Pensando que você vai pensar na cor como segunda pergunta para a melancia, e descarta ela caso seja menor que 50 cm de diâmetro, qual ponto de decisão que te faz escolher a melancia ou não? Ela pode ser amarela ou então ou verde mais claro? Essa informação do que você tolera, e do que não, faz você decidir certo? Pois bem, esse ponto que faz você querer ou não querer mais a melancia é justamente o que no mundo dos \"robôs\" se chama \"threshold\", que é o ponto de corte entre, \"quero essa melancia\" e \"não quero essa melancia\". Imaginando que a melancia é verde escuro, e está dentro das cores que você aceitaria, você responde essa pergunta positivamente e isso faz com que você queira comprar a melancia, chegamos novamente em uma folha da árvore, e a decisão nela é comprar.\n",
    "\n",
    "Recapitulando então, temos duas perguntas, a primeira: O diâmetro da melancia é maior que 50 cm? e a segunda: A cor da melancia é verde escuro?. Quando nos perguntamos sobre o diâmetro, caso a resposta seja sim, nos perguntamos a segunda pergunta, e caso a resposta seja não, descartamos a melancia. Caso a segunda pergunta seja respondida com sim, compramos a melancia, caso seja com não, descartamos ela. Isso configura uma árvore de 2 níveis, que contém 3 folhas, porque, ou decidimos descartar na primeira pergunta, ou comprar na resposta afirmativa da segunda pergunta, ou descartar novamente na segunda pergunta. Essa árvore pode ser desenhada mais ou menos assim:\n",
    "\n",
    "![](arvore_decisao.png \"Desenho Árvore de Decisão\")\n",
    "\n",
    "Um fato é, quanto mais perguntas nós nos fazemos, melhor escolhemos as melancias certo? Então, quanto mais perguntas você pedir para o robô, mais preciso ele será. Porém, se fizermos várias perguntas, é capaz de especificarmos muito o processo de escolha feito pelo robô, o que talvez levaria ele a ser tão seletivo, mais tão seletivo que nunca encontraria uma melancia para comprar. Esse é o problema que temos com essas \"Árvores de decisão\" cheias de perguntas (cheias de níveis), porque quanto mais níveis, mais seletivo ele é. Além dos níveis e folhas (perguntas e decisões), outras partes mais complexas do robô que também existem são algumas barreiras, restrições que você pode colocar para deixar ele mais complexo. Por exemplo, podemos pedir para ele não ter mais de 5 níveis de perguntas (max_depth) então limitamos o robô a se perguntar 5 perguntas consecutivas e depois precisa escolher. Ou então dentre as várias melancias que ele analisar, pelo menos 10 melancias tem que estar em cada \"folha\" da árvore (min_samples_leaf), ele não pode colocar como ponto de decisão, um ponto que dentre as várias melancias que ele analisou, menos de 10 estejam nesse ponto de decisão.\n",
    "\n",
    "A última coisa vô, e o senhor estará sabendo tudo sobre o tal robô seletor de melancias. Eu menti sobre quem fala o que o robô deve fazer... não é o senhor que fala exatamente os thresholds, aqueles pontos de decisão entre responder sim ou não a pergunta. O robô é inteligente o suficiente para descobrir quais são as perguntas mais importantes, como elas devem ser respondidas, e quando ele deve perguntar novamente algo sobre a melancia, ou se ele já está convencido o suficiente para decidir se vai comprar ou não ela. Você na verdade apenas da as melancias para ele analisar e fala, essa é boa, e essa é ruim. A forma como ele decide é pensando em separar as melancias boas das melancias ruins, toda vez que ele pensa em uma pergunta, ele usa ela entre as melancias que ele está olhando, se essa pergunta gerar dois grupos de melancias menos diversos, ou seja, com menos \"mistureba\" entre melancias boas e ruins, ele vai escolher essa pergunta, caso contrário ele continua procurando novas perguntas. Toda vez que ele decide que a pergunta é a melhor, é porque ela vai dividir melhor as melancias entre boas e ruins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação 2\n",
    "\n",
    "Tópico: Bagging e boosting: para que servem, porque funcionam, qual a diferença entre eles?\n",
    "\n",
    "Interlocutor: Um colega do Insper espertinho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então Carmona, lembra o conceito de árvore de decisão? Aquele modelo que vai tomando decisões de acordo com perguntas e tudo mais? Então, a gente pode usar ele para prever inandimplências das empresas em seus acordos de dívida né? Você já fez isso alguma vez? Eu estava fazendo isso na minha eletiva, com um dataset que eu encontrei, e cara, ele errou muito nesses casos. Ai eu fiquei decepcionado e fui atrás de como melhorar ele, e descobri dois outros modelos que usam da árvore de decisão, mas de uma forma diferente, o Bagging e o Boosting, eles basicamente usam várias árvores de decisão e decidem a partir delas.\n",
    "\n",
    "O Bagging funciona como um emaranhado de árvores de decisão treinadas e prevendo sobre o dataset que você tem, porém cada árvore tem acesso a uma amostra do nosso dataset. O funcionamento dessas amostras é um pouco contra intuitivo, mas apresenta um resultado muito bom. Nós escolhemos quantas árvores queremos treinar, e sorteamos amostras com reposição delas e treinamos essas árvores por completo, com todas os hiperparâmetros pré setados por nós. Isso vai gerar várias árvores com um bom desepempenho, afinal, antes disso treinamos e selecionamos os melhores hiperparâmetros para a árvore. Dessa forma, os erros de cada árvore com um número grande de árvores envolvidas na decisão final vão acabar reduzindo, de modo que, quando utilizamos os resultados de todas elas para realmente prever o que queremos, seja pela média dos resultados entre elas, ou sorteando aleatoriamente algumas delas para decidir, diminuímos o erro da previsão. De uma forma bem besta, mas acho que funciona, lembra da aula de diversificação de carteira de Markovitz em Finanças 2? Então, ao escolher vários ativos, com diferentes setores, e diferentes variâncias e covariâncias, o risco delas na carteira como um todo reduz e temos um portfólio mais estável e mais rentável. Pensando ainda nesse exemplo das carteiras, a carteira final possui uma variância menor que a maioria dos ativos que a compõe certo? Por justamente estarmos diversificando os ativos. Então, a ideia do Bagging é exatamente essa, ao utilizar vários modelos treinados em diferentes amostras do nosso dataset, focamos em reduzir a variância da previsibilidade do nosso modelo, claro, desde que a correlação entre as observações do nosso dataset sejam pequenas, porque se forem altas nunca conseguiremos \"mitigar o risco da carteira\". A parte boa do Bagging é que com ele você pode utilizar árvores mais profundas, que por reduzirmos a variância ao aplicarmos a média dos modelos, podemos na contraparte, aumentar a variância individual de cada modelo, deixando a árvore mais livre com relação a sua profundidade.\n",
    "\n",
    "O Boosting tem a mesma ideia de várias árvores de decisão olhando pro dataset, mas a forma como os dados são usados em cada árvore é um pouco diferentes. O que ele faz é usar um modelo, prever e testar ele, encontrando os erros/resíduos que o modelo gerou. Ai depois disso, ele pega outro modelo e treina ele mas dando um peso maior nas variáveis que geraram erros no modelo anterior. E isso acontece sucessivamente, assim, o que temos são vários modelos treinados para prever a inadimplência, mas toda vez treinamos eles buscando melhorar o desempenho do modelo exatamente onde o anterior errou. No final então, temos vários modelos que juntos se completam, porque o erro de um o outro busca não cometer, e assim se constrói um grupo de modelos que juntos são muito fortes. Um detalhes interessante sobre a parte de processamento interno dos modelos, é que o Bagging só junta as árvores no final, então o que o computador faz por trás dos panos é treinar todas as árvores ao mesmo tempo e depois juntar elas para prever. Já aqui no Boosting, como cada modelo depende dos erros do anterior, o computador faz um de cada vez, sempre utilizando os resultados do anterior para treinar o próximo, o que pode demorar mais. Juntando esses vários modelos corrigindo suas variáveis, o resultado final é um modelo muito preciso, em que está muito preocupado com seus erros sobre o dataset de treino, então o que ele prevalece durante suas previsões é justamente aprender o viés do dataset, diferentemente do Bagging que olha mais para a variância. Isso quer dizer que alterar o dataset, por exemplo, mudar o país em que estamos analisando essas inadimplências, pode causar grandes estragos nos resultados encontrados, visto que ao mudarmos o cenário macro em que essas empresas estão inseridas a variância dos dados será altíssima, algo que o modelo de Boosting não sabe lidar muito bem. Por causa disso, quando decidimos os hiperparâmetros das árvores que serão treinadas sequencialmente, é preferível utilizarmos mais árvores rasas do que menos árvores profundas, já que nosso modelo consegue pelo conjunto das árvores reduzir o viés e não a variância. Utilizando várias árvores rasas nós nos blindamos de certo modo contra a variância dos dados que vamos utilizar para prever no futuro.\n",
    "\n",
    "No final das contas, os dois modelos brincam com os dois aspectos da árvore de decisão (variância e viés) e exploram da melhor forma cada um. Por um lado podemos ser generalistas e nos diversificar sobre os modelos como uma carteira de Markovitz, e por outro podemos ser especialistas na análise de crédito de FIDCs de precatório, algo super específico. Espero que um dia possamos utilizar algum desses modelos no trabalho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação 3\n",
    "\n",
    "Tópico: A importância da redução de dimensionalidade e o algoritmo PCA\n",
    "\n",
    "Interlocutor: Você mesmo do início do semestre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucca, lembra daquela aula de PCA, que você faltou na eletiva de modelagem preditiva avançada, então... vamos entender ela e o porquê da sua importância? Falando primeiramente sobre a importância do objetivo do PCA, redução da dimensionalidade, ela se faz importante principalmente em datasets com muitas features, pois quanto maior a quantidade de variáveis disponíveis no dataset, maior a dimensionalidade do espaço em que os dados estão dispostos, de forma que as distâncias entre as observações passam a ter menor relevância visto que elas ficam mais parecidas, dificultando os modelos de utilizarem tais distâncias para treinar e prever objetivos. Além disso, para modelos como regressão linear, a colinearidade é um problema gravíssimo e quando trabalhamos com muitas variáveis, a chance de termos variáveis reduntantes é muito alta. Ademais, treinar modelos com muitas variáveis demora mais tempo, pois os modelos precisam analisar muito mais aspectos o que aumenta o tempo necessário, dessa forma, reduzindo essas variáveis para apenas as mais importantes e mais explicativas, trabalhamos esse problema e ao mesmo tempo, aplicamos uma regularização indiretamente, matando \"dois coelhos em uma cajadada só\".\n",
    "\n",
    "Resumido da forma mais direta a importância de reduzirmos a dimensionalidade em alguns casos, vamos passar para o funcionamento do famoso modelo Principal Components Analysis, aquele modelo que todos seus amigos economistas ficam te zuando que você não aprendeu, por não ter feito econometria avançada. Indo direto ao ponto, passo a passo como o modelo funciona e o que precisa ser feito para ele ser efetivo, o primeiro passo aqui é normalizar todas as features, subtraindo a média e dividindo pelo seu desvio padrão, afinal, estamos tratando de um modelo espacial, que lida com distâncias, e o cuidado entre variáveis de bilhões e variáveis de milésimos se faz necessário. O próximo passo é olhar todas as observações existentes no dataset e testar novas direções que consigam captar a maior quantidade de variância das observações. Matematicamente é algo um pouco complexo mesmo, mas é possível entender melhor com o seguinte desenho:\n",
    "\n",
    "![](pca.png \"PCA\")\n",
    "\n",
    "Nesse gráfico, qual eixo consegue explicar melhor as observações presentes entre esses dois eixos t(1) ou t(2)? Se pegarmos todos os pontos e colarmos em ambos os eixos, você concorda que no eixo t(2) todos os pontos ficam muito juntos o que quebra a ideia do modelo PCA, que é justamente dar um novo ponto de vista para as observações, e uma nova importância às distâncias, perante novos eixos? Então, o eixo t(1), consegue ter as observações melhor distribuidas por sua extensão, com distâncias mais concretas entre os pontos, distâncias essas que poderão ser utilizadas no futuro por outros modelos. De forma geral, esse tão complexo e famoso modelo de redução de demensionalidade, o PCA, se constitui nisso, olhar qual direção/eixo, os pontos são melhores distribuídos, e melhor explicados.\n",
    "\n",
    "Agora, com suas novas features (os eixos que você utilizou para explicar suas observações), podemos utilizar elas em treinamentos de outros modelos para alcançar resultados melhores perante aos seus objetivos. É interessante de mencionar que esses eixos, chamados de direções principais, possuem noção de importância, sendo o eixo com maior quantidade de variância explicada, o mais importante para transparecer as informações presentes no dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicação 4\n",
    "\n",
    "Tópico: Machine learning e desigualdade social\n",
    "\n",
    "Interlocutor: Seu professor de história do colegial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sim Fefê, mais uma vez o capitalismo conseguiu achar um instrumento segregacionista e na maioria das vezes preconceituoso. Ignorando a parte matemática e computacional, o machine learning, ou aprendizado de máquina em português, nada mais é do que o poder computacional disponível hoje, empregado sobre diversos dados, utilizando matemática e buscando prever o futuro. O que estamos fazendo hoje, e quando digo estamos, me refiro a todo o mundo, principalmente os maiores agentes capitalistas participantes dos mercados internos e externos, aqueles que \"constroem\" o PIB, é lendo o passado que foi gravado em formato de números, fotos, vídeos e até mesmo som, e utilizando desses dados para entender o que podemos esperar para as próximas horas, dias, meses, anos ou até décadas.\n",
    "\n",
    "Como você sempre falava em suas aulas, o capitalismo nada mais é do que um sistema que segrega populações e funciona a partir de uma engrenagem estática, onde \"o de cima sobe e o de baixo desce...\" como diz aquela música que você sempre cantava \"Xibom Bombom\". E por conta desse funcionamento estar predominante no mundo após a Segunda Guerra Mundial, e a internet ter sido criada após esse momento histórico, a maior parte dos dados históricos amplamente utilizados hoje em dia nos modelos de aprendizado de máquina, tem como base, o viés do sistema capitalista instaurado em toda e qualquer situação social. Assim, como o objetivo do machine learning é olhar os dados e prever o futuro - podendo reestruturar essa frase para, \"olhar o passado e prever o futuro\". Sendo o passado desigual, estamos adicionando mais uma ferramenta para o capitalismo continuar sendo cada vez mais desigual. Isto pois, por exemplo, vamos pensar que no passado bancos não forneciam muito crédito às mulheres, por considerar elas economicamente instáveis. O modelo de machine learning observando isso, vai aprender que mulheres não devem receber tanto crédito como homens, por apresentarem um risco em média maior para os bancos. Ou então algo mais problemático ainda, citando uma notícia super recente. Estudos da medicina muitas vezes utilizam de previsões e construção de cenários envolvendo dados históricos, porém esses dados históricos muitas vezes são limitados, principalmente quando se trata de pessoas e tratamentos médicos. Pelo fato de em países sub desenvolvidos, com populações predominantemente com etnias afro-descendentes não contarem com muitos recursos, inclusive de internet e dados, esses povos são menos representados quando são criados esses cenários enos estudos de medicina, algo que impacta diretamente a descoberta de fatos e tratamentos específicos, por exemplo, para pessoas afro-descendentes.\n",
    "\n",
    "Além do exemplo do crédito, você já deve estar pensando em vários outros, inclusive nesse que também citarei. Muitas vezes o policiamento hoje em dia é suportado por diversas tecnologias que apoiam e facilitam o trabalho dos policiais. No entanto, por conta do viés presente na base de dados policial com relação a quem foi indiciado por cometer crimes, pessoas pobres e afro-descendentes, de acordo com a base de dados policial, possuem maior probabilidade de cometer crimes. Assim, algoritmos como por exemplo, utilizados na seleção dos passageiros que precisarão passar por uma revista mais apurada no momento de imigração, pode acusar a seleção maior de pessoas afro-descendentes do que pessoas brancas. E o paradoxo mais preocupante, é de que essa aceleração dos viéses construídos ao longo do passado não terá fim, porque cada vez mais utilizamos machine learning nos momentos de decisão, o que gera dados enviesados e cheios de preoconceitos. Dados esses que serão usados no futuro, para prever o futuro do futuro, que gerarão novos dados enviesados que serão utilizados para o futuro do futuro do futuro, ou seja, uma linha do tempo sem fim, que não tem muitas formas de ser interrompida.\n",
    "\n",
    "Se eu me lembro bem das suas aulas, e entendi seus principais ensinamentos, em casos como esse, que o distúrbio é global, envolvendo diversas nações e pessoas, cabe aos governos se juntarem e auxiliarem em como a sociedade mundial deve seguir perante a tais fatos. É um fato que hoje caso ninguém atue sobre isso, nós estamos apenas acelerando desigualdade a social, e por isso, os governos deveriam se unir e discutir, quais as melhores formas de controle sobre essa engrenagem, seja por regularização desses processos, ou até mesmo cosntrução de fóruns que unam os dois polos dessa divergência, os times de dados do mercado, e historiadores preocupados com o crescimento desenfreado da desigualdade mundial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
